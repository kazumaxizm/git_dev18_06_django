{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トレンドレコメンド（仮）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 世間で話題になっているトレンドを抽出し、トレンドに関連した商品を自動抽出する\n",
    "- 使用したAPI （TwitterAPI, NewsAPI, 楽天API）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 処理の流れ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 以下のいずれかの方法により、トレンドワードを取得する\n",
    "- GoogleトレンドのWebページをクローリング・スクレイピングし、Google検索で急上昇中のワードを取得する（https://trends.google.co.jp/trends/trendingsearches/realtime?geo=JP&category=all）\n",
    "- TwitterAPIを使用し、Twitterでトレンドになっているワードを取得する\n",
    "- NewsAPIを使用し、国内ニュースでトレンドになっているニュースを取得する（これはワードではなく、ニュース記事を取得）\n",
    "<br>\n",
    "※ 各処理で必要となる処理は関数化済みだが、今回はGoogleトレンドのパターンで実験する\n",
    "\n",
    "\n",
    "2. トレンドワードに関連するニュース記事をNewsAPIを使用して取得する\n",
    "- 1で取得したトレンドワードでニュース記事を検索する\n",
    "- ニュース記事をタイトルと概要を取得する（本文取得はAPIではできない）\n",
    "\n",
    "3. トレンドワードに関連するツイートをTwitterAPIを使用して取得する\n",
    "- 取得したトレンドワードでツイートを検索する\n",
    "\n",
    "4. トレンドワード・ニュース記事（複数）・ツイート（10件）を一つのテキストにまとめる\n",
    "5. 楽天APIを使用し、楽天ブックスで販売する書籍のタイトル・著者・概要を取得する\n",
    "- 一度に楽天APIで取得できる商品数には制限がある為、事前にAPIを一日叩き続けて取得できた10万件近い書籍のデータを使用する\n",
    "- 書籍データはpandasのデータとして保持し、pickleファイル化しておく（実際はDBに保存しておきたい）\n",
    "\n",
    "6. 5で取得した書籍情報（タイトル・著者・概要）を一つのテキストにまとめる\n",
    "\n",
    "7. 4と6をMecabで形態素解析し、名詞だけを取得する\n",
    "\n",
    "8. TFIDFのベクトル化して、トレンドに関連するテキスト（トレンドワード・ニュース記事・ツイート）に一番類似する商品をCOS類似度を算出し求める"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下、処理内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインストール（AWSで環境をゼロから作り直す時必要）\n",
    "# !pip install newsapi-python\n",
    "# !pip install twitter\n",
    "# !pip install selenium\n",
    "# !pip install feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各APIで必要なライブラリをインポート\n",
    "from newsapi import NewsApiClient\n",
    "from twitter import *\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# その他、必要なライブラリをインポート\n",
    "import datetime\n",
    "import re\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import feedparser\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.ini']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API_KEYの設定ファイルを読み込み\n",
    "# ※ APIを使用する為に必要となるキーとパスをまとめた設定ファイルを別途作成し、それを呼び出している\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postgresqlに接続する為のライブラリ\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### news_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_apiから、検索したキーワードに関連するニュース情報を返す関数\n",
    "def keyword_news_search(\\\n",
    "                         api_key = config['NEWS_API']['KEY'],\\\n",
    "                         search_word = '', \\\n",
    "                         start_date = datetime.datetime.now().strftime('%Y-%m-%d'),\\\n",
    "                         end_date = datetime.datetime.now().strftime('%Y-%m-%d'), \\\n",
    "                         sort_type = 'popularity'\\\n",
    "                        ):\n",
    "    \n",
    "    # news_apiを初期化\n",
    "    news_api_search = NewsApiClient(api_key=config['NEWS_API']['KEY'])\n",
    "    \n",
    "    # search_wordで指定したキーワードに関するニュースを検索する\n",
    "    result_search_news = news_api_search.get_everything(qintitle=search_word, from_param=start_date ,to=end_date, sort_by=sort_type)\n",
    "    \n",
    "    return result_search_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_apiから、トップニュース情報を返す関数\n",
    "def headlines_news(\\\n",
    "                         api_key = config['NEWS_API']['KEY'],\\\n",
    "                         target_category = 'entertainment', \\\n",
    "                         target_country = 'jp'\n",
    "                        ):\n",
    "    \n",
    "    # news_apiを初期化\n",
    "    news_api_headlines = NewsApiClient(api_key=config['NEWS_API']['KEY'])\n",
    "    \n",
    "    # 指定したオプションに従ったトップニュースを検索する     \n",
    "    # Possible options: business entertainment general health science sports technology . \n",
    "    # Note: you can't mix this param with the sources param.\n",
    "    result_headlines_news = news_api_headlines.get_top_headlines(category=target_category, country=target_country)\n",
    "    \n",
    "    return result_headlines_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### twitter_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_apiから、ツイートトレンドの単語を返す関数\n",
    "def twitter_trends_search(\\\n",
    "                          CK = config['TWITTER_API']['CONSUMER_KEY'],\\\n",
    "                          CS = config['TWITTER_API']['CONSUMER_SECRET'],\\\n",
    "                          AT = config['TWITTER_API']['ACCESS_TOKEN'],\\\n",
    "                          AS = config['TWITTER_API']['ACCESS_TOKEN_SECRET'],\\\n",
    "                          area_id = 23424856\\\n",
    "                         ):\n",
    "    \n",
    "     # twitter_apiを初期化\n",
    "    twitter_api_trends = Twitter(auth = OAuth(AT,AS,CK,CS))\n",
    "        \n",
    "     # 指定したエリアのツイッタートレンドを検索する\n",
    "    twitter_trends_results = twitter_api_trends.trends.place(_id = area_id)\n",
    "                    \n",
    "    return twitter_trends_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_apiから、検索したキーワードに関連するツイートを返す関数\n",
    "def twitter_search(\\\n",
    "                          CK = config['TWITTER_API']['CONSUMER_KEY'],\\\n",
    "                          CS = config['TWITTER_API']['CONSUMER_SECRET'],\\\n",
    "                          AT = config['TWITTER_API']['ACCESS_TOKEN'],\\\n",
    "                          AS = config['TWITTER_API']['ACCESS_TOKEN_SECRET'],\\\n",
    "                          search_keyword = 'twitter',\\\n",
    "                          search_lang = 'ja',\\\n",
    "                          search_type = 'mixed',\\\n",
    "                          search_count = 100\\\n",
    "                         ):\n",
    "    \n",
    "     # twitter_apiを初期化\n",
    "    twitter_api_search = Twitter(auth = OAuth(AT,AS,CK,CS))\n",
    "        \n",
    "     # 指定したエリアのツイッタートレンドを検索する\n",
    "    search_results = twitter_api_search.search.tweets(q=search_keyword, lang=search_lang, result_type=search_type, count=search_count)\n",
    "                    \n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Googleトレンド（クローリング・スクレイピング）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_trends_search(\\\n",
    "                        base_url = 'https://trends.google.co.jp/trends/trendingsearches/realtime',\\\n",
    "                        search_geo = 'JP',\\\n",
    "                        search_category = 'all',\n",
    "                        ):\n",
    "    \n",
    "    try:\n",
    "\n",
    "        # 検索するURLを作成する\n",
    "        url = '{i}?geo={j}&category={k}'.format(i=base_url, j=search_geo, k=search_category)\n",
    "\n",
    "        # ブラウザを開く\n",
    "        driver = webdriver.Chrome()\n",
    "\n",
    "        # Googleトレンドの検索TOP画面を開く （表示サイズも指定しておく）\n",
    "        driver.get(url)\n",
    "        driver.set_window_size(2500,1000)\n",
    "\n",
    "        # 2〜4秒間、ページが開かれるのを待つ\n",
    "        time.sleep(random.randint(2,4))\n",
    "\n",
    "        # TOP画面より、必要な要素のデータを取得する\n",
    "        trends_keywords = driver.find_elements_by_class_name('title')\n",
    "        summary_text = driver.find_elements_by_class_name('summary-text')\n",
    "        source_text = driver.find_elements_by_class_name('source-and-time')\n",
    "\n",
    "        # 全ての要素が取得できた場合、処理を進める\n",
    "        while bool(len(trends_keywords) == 0 or  len(summary_text) == 0 or len(source_text) == 0):\n",
    "            time.sleep(10)\n",
    "            trends_keywords = driver.find_elements_by_class_name('title')\n",
    "            summary_text = driver.find_elements_by_class_name('summary-text')\n",
    "            source_text = driver.find_elements_by_class_name('source-and-time')\n",
    "        else:\n",
    "            trends_keyword = []\n",
    "            trends_abst = []\n",
    "            inyo_url = []\n",
    "            inyo_site = []\n",
    "            for i in range(len(trends_keywords)):\n",
    "                trends_keyword.append(trends_keywords[i].text)\n",
    "                trends_abst.append(summary_text[i].text)\n",
    "                inyo_url.append(summary_text[i].find_element_by_tag_name(\"a\").get_attribute(\"href\"))\n",
    "                inyo_site.append(source_text[i].text.split()[0])\n",
    "\n",
    "        # 各トレンドごとに詳細情報を取得する為、要素をクリックし必要なHTMLを表示させる（その為のクリックする要素を取得している）\n",
    "        trend_click = driver.find_elements_by_class_name('feed-item')\n",
    "\n",
    "        # 蓄積用のリストを用意しておく\n",
    "        google_trend_news_list = []\n",
    "        google_trend_words_list = []\n",
    "\n",
    "        # トレンドごとに詳細情報を取得する為、要素をクリックし必要なHTMLを表示させる\n",
    "        for trend in trend_click:\n",
    "            trend.click()\n",
    "\n",
    "            # 1秒間、ページが開かれるのを待つ\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # 引用ニュースを取得\n",
    "            google_trend_news_tmp = driver.find_elements_by_class_name('item-title')\n",
    "            google_trend_news = ''\n",
    "            for news in google_trend_news_tmp:\n",
    "                google_trend_news = google_trend_news + news.text\n",
    "\n",
    "            # 関連ワードを取得\n",
    "            google_trend_words_tmp = driver.find_elements_by_class_name('list')\n",
    "            google_trend_words = google_trend_words_tmp[0].text.replace(\" \",\"\")\n",
    "\n",
    "            # リストに追加\n",
    "            google_trend_news_list.append(google_trend_news)\n",
    "            google_trend_words_list.append(google_trend_words) \n",
    "\n",
    "        driver.quit()\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    google_trend_data = {'goole_keyword': trends_keyword, 'google_abst': trends_abst, 'google_url': inyo_url, 'google_site': inyo_site, \\\n",
    "                        'google_news':google_trend_news_list, 'google_words':google_trend_words_list}\n",
    "    \n",
    "    return google_trend_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends = google_trends_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要な処理は用意できたので、ここからレコメンドを作っていく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ちょっとしたデータ加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trend_word = []\n",
    "for i in google_trends['goole_keyword']:\n",
    "    google_trend_word.append(i.split(' • '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trend_news = google_trends['google_news']\n",
    "google_trend_words = google_trends['google_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【NesAPIを使用】トレンドワードに基づいて、関連ニュースを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_news = []\n",
    "for j in google_trend_word:\n",
    "    sample_news.append(keyword_news_search(search_word = j[0]))\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_text = []\n",
    "for k in sample_news:\n",
    "    news_text_tmp = ''\n",
    "    for o in k['articles']:\n",
    "        news_text_tmp += (o['title'] +\",\"+ str(o['description']))\n",
    "    news_text.append(news_text_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【twitterAPIを使用】トレンドワードに基づいて、関連ツイートを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twtter_text = []\n",
    "for p in google_trend_word:\n",
    "    twtter_text.append(twitter_search(search_keyword=p[0], search_count = 10))\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_text_list = []\n",
    "for q in twtter_text:\n",
    "    \n",
    "    twitter_text_tmp = ''\n",
    "    for i in range(len(q[\"statuses\"])):\n",
    "        twitter_text_tmp += q[\"statuses\"][i][\"text\"]\n",
    "        \n",
    "    twitter_text_list.append(twitter_text_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### googleトレンドのページに記載されているテキストは重みをつける（✖️3）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　ちょっとデータ加工\n",
    "google_trend_word_text = []\n",
    "for r in google_trend_word:\n",
    "    google_trend_word_text_tmp = ','.join(r)\n",
    "    google_trend_word_text.append(google_trend_word_text_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = []\n",
    "for s in range(len(google_trend_word_text)):\n",
    "#     test_text.append((google_trend_word_text[s] + google_trend_news[s] + google_trend_words[s]) * 3 + news_text[s] + twitter_text_list[s])\n",
    "    test_text.append((google_trend_word_text[s] + google_trend_news[s] + google_trend_words[s]) * 3 + news_text[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 楽天APIで取得済みの商品リストを持ってくる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 楽天APIで書籍情報を取得し続ける処理は別途作成し実行ずみ。.pickleのファイルにしているので、ここで読み込み使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL Server へ接続（必要情報は削除）\n",
    "conn = psycopg2.connect('host=localhost port=5432 dbname=trend_books user=postgres password=panchi1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データベースの接続情報（必要情報は削除）\n",
    "connection_config = {\n",
    "    'user': 'postgres',\n",
    "    'password': 'panchi1',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432', # なくてもOK\n",
    "    'database': 'trend_books'\n",
    "}\n",
    "engine = create_engine('postgresql://{user}:{password}@{host}:{port}/{database}'.format(**connection_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQLに楽天APIから集計したデータを書き込む\n",
    "\n",
    "# import pickle\n",
    "# with open('df_all_duplicated.pickle','rb') as f:\n",
    "#     book_list = pickle.load(f)\n",
    "# book_list_db.to_sql('book', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQLにトレンド情報のデータを書き込む\n",
    "df_trend = pd.DataFrame({'time': pd.to_datetime(datetime.datetime.now(), format='%y%m%d %H:%M'),\\\n",
    "                         'rank': [i+1 for i in range(len(google_trend_news))],\\\n",
    "                         'google_trend_words': google_trend_word_text,\\\n",
    "                         'google_trend_news': google_trend_news,\\\n",
    "                         'google_trend_words_related': google_trend_words,\\\n",
    "                         'news': news_text,\\\n",
    "                         'tweet': twitter_text_list})\n",
    "\n",
    "df_trend.to_sql('trend', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQLに接続する\n",
    "connection = psycopg2.connect(**connection_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルの全レコードを格納する\n",
    "book_list = pd.read_sql(sql=\"SELECT * FROM book;\", con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_books = (book_list['title']+\",\"+book_list['author']+\",\")*5+\",\"+book_list['subTitle']+\",\"+book_list['itemCaption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame()\n",
    "df1['test'] = test_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(test_text, columns=['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = df1\n",
    "df3 = df1.append(df2).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自然言語処理で、テキストをベクトルに変換していく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import janome\n",
    "# from janome.tokenizer import Tokenizer\n",
    "\n",
    "# tokenizer = Tokenizer()\n",
    "\n",
    "# for token in tokenizer.tokenize(\"大村競艇\"):\n",
    "#     print(\"    \" + str(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeCabの辞書にnelogdを指定する\n",
    "m = MeCab.Tagger(r\"-r C:\\Users\\kazum\\Desktop\\GS\\trend_recommend\\database_test\\api\\mecabrc-u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# あまり関係のないと思われる数字を全て0に置き換える関数\n",
    "def replace_number_to_zero(text):\n",
    "    changed_text = re.sub(r'[0-9]+', \"0\", text) #半角\n",
    "    changed_text = re.sub(r'[０-９]+', \"0\", changed_text) #全角\n",
    "    return changed_text\n",
    "\n",
    "# 数字を0に置換\n",
    "df3['review_number_to_zero'] = df3['test'].map(replace_number_to_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 分かち書きした結果を返す関数\n",
    "def leaving_space_between_words_column(text):\n",
    "    \n",
    "    nouns = []\n",
    "        \n",
    "    for line in m.parse(text).splitlines():\n",
    "        if len(line.split()) > 0:\n",
    "            if \"名詞\" in line.split()[-1]:\n",
    "                nouns.append(line)\n",
    "                \n",
    "    meishi_list = []\n",
    "    for str in nouns:\n",
    "        try:\n",
    "#             meishi_list.append(str.split()[2])\n",
    "            meishi_list.append(str.split(\",\")[6])\n",
    "        except:\n",
    "            meishi_list.append(\"\")\n",
    "    \n",
    "#     return ','.join(meishi_list)\n",
    "    return ' '.join(meishi_list)\n",
    "\n",
    "# 分かち書きしたカラムをdfに追加する\n",
    "df3['lsbw'] = df3['review_number_to_zero'].map(leaving_space_between_words_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50万冊の書籍情報をベクトル化\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df3['lsbw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレンドのテキスト情報もベクトル化\n",
    "test_item = vectorizer.transform(df2[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 特徴語を取得する\n",
    "values = test_item.toarray()\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dfsss = pd.DataFrame(values, columns = feature_names)\n",
    "df_0 = dfsss.T\n",
    "\n",
    "feature_list = []\n",
    "\n",
    "for i in df_0.columns:\n",
    "    feature_list.append(df_0[df_0[i]>0][i].sort_values(ascending=False)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<トレンド情報>>\n",
      "米大リーグ歴代２位の７５５本塁打、ハンク・アーロンさん死去…８６歳\n",
      "-----\n",
      "<<関連書籍>>\n",
      "1\n",
      "【輸入楽譜】ベスト・オブ・ハンク・ウィリアムズ - 第2版,,【輸入楽譜】ベスト・オブ・ハンク・ウィリアムズ - 第2版,,【輸入楽譜】ベスト・オブ・ハンク・ウィリアムズ - 第2版,,【輸入楽譜】ベスト・オブ・ハンク・ウィリアムズ - 第2版,,【輸入楽譜】ベスト・オブ・ハンク・ウィリアムズ - 第2版,,,,\n",
      "2\n",
      "ウルフ・ウォーズ,ハンク・フィッシャー/朝倉裕,ウルフ・ウォーズ,ハンク・フィッシャー/朝倉裕,ウルフ・ウォーズ,ハンク・フィッシャー/朝倉裕,ウルフ・ウォーズ,ハンク・フィッシャー/朝倉裕,ウルフ・ウォーズ,ハンク・フィッシャー/朝倉裕,,オオカミはこうしてイエローストーンに復活した,市民、政治家、畜産業界、自然保護団体の思惑が複雑に絡み合う困難な道をどう切り開いてゴールを目指すか。オオカミ再導入実現の立役者が苦闘と創意工夫の２０年をつぶさに物語る。\n",
      "3\n",
      "【輸入楽譜】ウィリアムス, Hank: ハンク・ウィリアムス: Complete,ウィリアムス, Hank,【輸入楽譜】ウィリアムス, Hank: ハンク・ウィリアムス: Complete,ウィリアムス, Hank,【輸入楽譜】ウィリアムス, Hank: ハンク・ウィリアムス: Complete,ウィリアムス, Hank,【輸入楽譜】ウィリアムス, Hank: ハンク・ウィリアムス: Complete,ウィリアムス, Hank,【輸入楽譜】ウィリアムス, Hank: ハンク・ウィリアムス: Complete,ウィリアムス, Hank,,,\n",
      "-----\n",
      "-----\n",
      "<<トレンド情報>>\n",
      "坂上忍 昨年結婚、野呂佳代の「僕が婚姻届の証人です」夫の正体も明かし松本「知らんかった」\n",
      "-----\n",
      "<<関連書籍>>\n",
      "1\n",
      "別冊月刊真木よう子Noise,藤代 冥砂,別冊月刊真木よう子Noise,藤代 冥砂,別冊月刊真木よう子Noise,藤代 冥砂,別冊月刊真木よう子Noise,藤代 冥砂,別冊月刊真木よう子Noise,藤代 冥砂,,,\n",
      "2\n",
      "桐谷健太 2nd PHOTO BOOK 『 CHELSEA 』,関根虎洸/桐谷健太,桐谷健太 2nd PHOTO BOOK 『 CHELSEA 』,関根虎洸/桐谷健太,桐谷健太 2nd PHOTO BOOK 『 CHELSEA 』,関根虎洸/桐谷健太,桐谷健太 2nd PHOTO BOOK 『 CHELSEA 』,関根虎洸/桐谷健太,桐谷健太 2nd PHOTO BOOK 『 CHELSEA 』,関根虎洸/桐谷健太,,,自らも“旅”を趣味とする俳優・桐谷健太の“放浪”ドキュメンタリー。向かった先はアメリカ合衆国・ニューヨーク。役者としての顔とは違う表情を至近距離でとらえた珠玉のＰＨＯＴＯ　ＢＯＯＫが完成！真実の“顔”に迫る。\n",
      "3\n",
      "クイズ王（2）,フジテレビジョン,クイズ王（2）,フジテレビジョン,クイズ王（2）,フジテレビジョン,クイズ王（2）,フジテレビジョン,クイズ王（2）,フジテレビジョン,,,\n",
      "-----\n",
      "-----\n",
      "<<トレンド情報>>\n",
      "岩田剛典 “I Don't Like Mondays.” YU、幼馴染対談実現に「記念すべき日」\n",
      "-----\n",
      "<<関連書籍>>\n",
      "1\n",
      "名も無き世界のエンドロール,行成薫,名も無き世界のエンドロール,行成薫,名も無き世界のエンドロール,行成薫,名も無き世界のエンドロール,行成薫,名も無き世界のエンドロール,行成薫,,,ドッキリを仕掛けるのが生き甲斐のマコトと、それに引っかかってばかりの俺は、小学校時代からの腐れ縁だ。３０歳になり、社長になった「ドッキリスト」のマコトは、「ビビリスト」の俺を巻き込んで、史上最大の「プロポーズ大作戦」を決行すると言い出したー。一日あれば、世界は変わる。男たちの命がけの情熱は、彼女に届くのか？大いなる「企み」を秘めた第２５回小説すばる新人賞受賞作。\n",
      "2\n",
      "Sports-Car　Racing（vol．21）,Sports-Car　Racing　Gr,Sports-Car　Racing（vol．21）,Sports-Car　Racing　Gr,Sports-Car　Racing（vol．21）,Sports-Car　Racing　Gr,Sports-Car　Racing（vol．21）,Sports-Car　Racing　Gr,Sports-Car　Racing（vol．21）,Sports-Car　Racing　Gr,,Do　you　like　Sports-Car　Ra,\n",
      "3\n",
      "SING　LIKE　TALKING　30th　ANNIVERSARY　ISSUE,,SING　LIKE　TALKING　30th　ANNIVERSARY　ISSUE,,SING　LIKE　TALKING　30th　ANNIVERSARY　ISSUE,,SING　LIKE　TALKING　30th　ANNIVERSARY　ISSUE,,SING　LIKE　TALKING　30th　ANNIVERSARY　ISSUE,,,昨日まで、そして今日から,シング・ライク・トーキング、デビュー３０周年。今ここで、３人それぞれが徹底的に語るヒストリー。青森での同級生としての関係から始まったＳＬＴのストーリーを、佐藤竹善、西村智彦、藤田千章それぞれの超ロング・インタビューでじっくりとたどる記念本。総文字量は２０万字オーバー＋全アルバムのレビューや復刻写真も掲載！\n",
      "-----\n",
      "-----\n",
      "<<トレンド情報>>\n",
      "miHoYoの『崩壊3rd』がApp Store売上ランキングで327位→19位に急上昇 コラボ戦乙女にアスカ登場の「エヴァンゲリオン」コラボ開催で\n",
      "-----\n",
      "<<関連書籍>>\n",
      "1\n",
      "ASKA 書きおろし詩集,ASKA,ASKA 書きおろし詩集,ASKA,ASKA 書きおろし詩集,ASKA,ASKA 書きおろし詩集,ASKA,ASKA 書きおろし詩集,ASKA,,,音楽の未来、友へのメッセージ、ステージ後の光景、極私的な郷愁、風の調べ、日々の感慨…本人が自身の手で綴った！“詩歌の才人”ＡＳＫＡが贈る３５年ぶりの詩集。\n",
      "2\n",
      "『ヱヴァンゲリヲン新劇場版：序』全記録全集ビジュアルストーリー版・設定資料版,,『ヱヴァンゲリヲン新劇場版：序』全記録全集ビジュアルストーリー版・設定資料版,,『ヱヴァンゲリヲン新劇場版：序』全記録全集ビジュアルストーリー版・設定資料版,,『ヱヴァンゲリヲン新劇場版：序』全記録全集ビジュアルストーリー版・設定資料版,,『ヱヴァンゲリヲン新劇場版：序』全記録全集ビジュアルストーリー版・設定資料版,,,,\n",
      "3\n",
      "STAGEA アーチスト 6〜3級 Vol.36 826aska 『DEPARTURE』,,STAGEA アーチスト 6〜3級 Vol.36 826aska 『DEPARTURE』,,STAGEA アーチスト 6〜3級 Vol.36 826aska 『DEPARTURE』,,STAGEA アーチスト 6〜3級 Vol.36 826aska 『DEPARTURE』,,STAGEA アーチスト 6〜3級 Vol.36 826aska 『DEPARTURE』,,,,\n",
      "-----\n",
      "-----\n",
      "<<トレンド情報>>\n",
      "元欅坂46今泉佑唯ファンがワタナベマホトとの結婚を祝福できない理由\n",
      "-----\n",
      "<<関連書籍>>\n",
      "1\n",
      "にゃにゃにゃの本〜YouTuber・ワタナベマホトと猫たち〜,,にゃにゃにゃの本〜YouTuber・ワタナベマホトと猫たち〜,,にゃにゃにゃの本〜YouTuber・ワタナベマホトと猫たち〜,,にゃにゃにゃの本〜YouTuber・ワタナベマホトと猫たち〜,,にゃにゃにゃの本〜YouTuber・ワタナベマホトと猫たち〜,,,,Ｔｗｉｔｔｅｒフォロワー数１００万人超！！大人気ＹｏｕＴｕｂｅｒ「ワタナベマホト」と愛する６匹の猫たちのフォトブックが登場！！！！！！お宝写真満載「マホトと猫のヒストリー」。マホトロングインタビュー＆猫動画ひと言コメント。おまめサンシロー描き下ろし「猫４コマ」\n",
      "2\n",
      "【楽天限定特典付き】欅坂46 今泉佑唯ソロ写真集「誰も知らない私」,今泉佑唯,【楽天限定特典付き】欅坂46 今泉佑唯ソロ写真集「誰も知らない私」,今泉佑唯,【楽天限定特典付き】欅坂46 今泉佑唯ソロ写真集「誰も知らない私」,今泉佑唯,【楽天限定特典付き】欅坂46 今泉佑唯ソロ写真集「誰も知らない私」,今泉佑唯,【楽天限定特典付き】欅坂46 今泉佑唯ソロ写真集「誰も知らない私」,今泉佑唯,,,\n",
      "3\n",
      "欅坂46 今泉佑唯ソロ写真集「誰も知らない私」,今泉 佑唯/中村 和孝,欅坂46 今泉佑唯ソロ写真集「誰も知らない私」,今泉 佑唯/中村 和孝,欅坂46 今泉佑唯ソロ写真集「誰も知らない私」,今泉 佑唯/中村 和孝,欅坂46 今泉佑唯ソロ写真集「誰も知らない私」,今泉 佑唯/中村 和孝,欅坂46 今泉佑唯ソロ写真集「誰も知らない私」,今泉 佑唯/中村 和孝,,,欅坂４６・今泉佑唯が、卒業前に、ハタチ前に見せた姿がここに。ずっと見たかった笑顔、今まで見たことがなかった大人びた表情、息を飲むほどのグラミーなボディ、、、いつまでも目が離せない“ずーみん”を満載。イタリア・ナポリ郊外のヴィラで暮らすように撮影をしました。今までと、この先の今泉佑唯を知るロングインタビューつき。\n",
      "-----\n",
      "-----\n",
      "<<トレンド情報>>\n",
      "アイナ・ジ・エンド、「THE FIRST TAKE」第86回にて“オーケストラ”を披露\n",
      "-----\n",
      "<<関連書籍>>\n",
      "1\n",
      "猫〜THE　FIRST　TAKE　ver．,,猫〜THE　FIRST　TAKE　ver．,,猫〜THE　FIRST　TAKE　ver．,,猫〜THE　FIRST　TAKE　ver．,,猫〜THE　FIRST　TAKE　ver．,,,PIANO　SOLO・PIANO　＆　VOCAL,\n",
      "2\n",
      "ROLANDALE　FIRST　BOOK,,ROLANDALE　FIRST　BOOK,,ROLANDALE　FIRST　BOOK,,ROLANDALE　FIRST　BOOK,,ROLANDALE　FIRST　BOOK,,,,\n",
      "3\n",
      "MINECRAFT公式ガイド　ネザー＆ジ・エンド,Mojang　AB,MINECRAFT公式ガイド　ネザー＆ジ・エンド,Mojang　AB,MINECRAFT公式ガイド　ネザー＆ジ・エンド,Mojang　AB,MINECRAFT公式ガイド　ネザー＆ジ・エンド,Mojang　AB,MINECRAFT公式ガイド　ネザー＆ジ・エンド,Mojang　AB,,MOJANG公式本,危険に満ちたネザーやジ・エンドの次元で生き残るのは簡単ではありません。オーバーワールドに無事に戻ってきたければレベルアップが必要です。「Ｍｉｎｅｃｒａｆｔ公式ガイド　ネザー＆ジ・エンド」は、異次元の世界のガイドマップです。ここにしかないＭｏｂとの対決方法や、貴重なブロックやアイテムがある場所も学べます。ＭＯＪＡＮＧのエキスパートが教えてくれる情報やヒントも満載。ネザーやジ・エンドを生き抜くための、まさに決定版です。\n",
      "-----\n",
      "-----\n",
      "<<トレンド情報>>\n",
      "応援購入サービス「Makuake」、「オンライン催事」機能の提供を開始！ ～第一弾は大丸東京店セレクトの銘菓子店を集めた「オンライン催事・大丸東京店 ...\n",
      "-----\n",
      "<<関連書籍>>\n",
      "1\n",
      "スゴ編。,編集者．jp,スゴ編。,編集者．jp,スゴ編。,編集者．jp,スゴ編。,編集者．jp,スゴ編。,編集者．jp,,カリスマ編集者から学ぶ7つの仕事力,つい読みたくなる本の仕掛け。ヒット作の裏側、お見せします。スゴイ編集者（＝スゴ編）の７つの仕事力。\n",
      "2\n",
      "My　Favorite　BREAKFAST,朝時間．jp,My　Favorite　BREAKFAST,朝時間．jp,My　Favorite　BREAKFAST,朝時間．jp,My　Favorite　BREAKFAST,朝時間．jp,My　Favorite　BREAKFAST,朝時間．jp,,かんたん・おいしい朝食レシピ,大人気サイト・朝時間．ｊｐの「みんなの朝ごはんレシピ」から厳選しました！すぐに作れる１７０品。\n",
      "3\n",
      "【バーゲン本】帯の伊勢丹　模様の伊勢丹,飛田　健彦,【バーゲン本】帯の伊勢丹　模様の伊勢丹,飛田　健彦,【バーゲン本】帯の伊勢丹　模様の伊勢丹,飛田　健彦,【バーゲン本】帯の伊勢丹　模様の伊勢丹,飛田　健彦,【バーゲン本】帯の伊勢丹　模様の伊勢丹,飛田　健彦,,,\n",
      "-----\n",
      "-----\n",
      "<<トレンド情報>>\n",
      "“コンプライアンス”違反を指摘する視聴者からの手紙が引き金となり、過剰な役柄の滝藤賢一は降板させられる事態に...：バイプレイヤーズ\n",
      "-----\n",
      "<<関連書籍>>\n",
      "1\n",
      "ぷしゅぷしゅどこいった？,テレビ東京,ぷしゅぷしゅどこいった？,テレビ東京,ぷしゅぷしゅどこいった？,テレビ東京,ぷしゅぷしゅどこいった？,テレビ東京,ぷしゅぷしゅどこいった？,テレビ東京,,,\n",
      "2\n",
      "東京カレンダー倶楽部,,東京カレンダー倶楽部,,東京カレンダー倶楽部,,東京カレンダー倶楽部,,東京カレンダー倶楽部,,,死ぬまでに食べたい50店！,\n",
      "3\n",
      "人生を切り拓く人のチャンスのつかみ方,東京カレンダー,人生を切り拓く人のチャンスのつかみ方,東京カレンダー,人生を切り拓く人のチャンスのつかみ方,東京カレンダー,人生を切り拓く人のチャンスのつかみ方,東京カレンダー,人生を切り拓く人のチャンスのつかみ方,東京カレンダー,,,ビジネス書籍では語られることの少ない幼少期や社会人としての原体験をクローズアップし、成功の秘訣を紐解く。リーダーを目指したい人必読の一冊！\n",
      "-----\n",
      "-----\n",
      "<<トレンド情報>>\n",
      "IBMの第4四半期売上高は予想下回る ソフト事業が不振\n",
      "-----\n",
      "<<関連書籍>>\n",
      "1\n",
      "巨象IBMに挑む,田島一,巨象IBMに挑む,田島一,巨象IBMに挑む,田島一,巨象IBMに挑む,田島一,巨象IBMに挑む,田島一,,,解雇自由を許さない！「人としての尊厳」を胸に立ち向かう組合員の闘いを描く渾身のルポ。\n",
      "2\n",
      "IBMの思考とデザイン,山崎 和彦/工藤 晶,IBMの思考とデザイン,山崎 和彦/工藤 晶,IBMの思考とデザイン,山崎 和彦/工藤 晶,IBMの思考とデザイン,山崎 和彦/工藤 晶,IBMの思考とデザイン,山崎 和彦/工藤 晶,,,「Ｇｏｏｄ　Ｄｅｓｉｇｎ　ｉｓ　Ｇｏｏｄ　Ｂｕｓｉｎｅｓｓ」の思想、実践、未来を語る決定版。岐路に立つ経営者、スタートアップや新規事業に関わるビジネスパーソン、チームにデザインの思考を取り入れたいマネージャー、時代の半歩先を捉える未来のデザイナーのための必読書。\n",
      "3\n",
      "IBM流必ず合意形成できる最強の会議術,小山太一,IBM流必ず合意形成できる最強の会議術,小山太一,IBM流必ず合意形成できる最強の会議術,小山太一,IBM流必ず合意形成できる最強の会議術,小山太一,IBM流必ず合意形成できる最強の会議術,小山太一,,,\n",
      "-----\n",
      "-----\n",
      "<<トレンド情報>>\n",
      "邪悪と狂気に満ちる村『バイオハザード ヴィレッジ』5.8発売決定 トレーラー＆キャラ写真公開\n",
      "-----\n",
      "<<関連書籍>>\n",
      "1\n",
      "あなたの自宅を明日から『ダイエット★ヴィレッジ』にする本,日本テレビ『ダイエット★ヴィレッジ』,あなたの自宅を明日から『ダイエット★ヴィレッジ』にする本,日本テレビ『ダイエット★ヴィレッジ』,あなたの自宅を明日から『ダイエット★ヴィレッジ』にする本,日本テレビ『ダイエット★ヴィレッジ』,あなたの自宅を明日から『ダイエット★ヴィレッジ』にする本,日本テレビ『ダイエット★ヴィレッジ』,あなたの自宅を明日から『ダイエット★ヴィレッジ』にする本,日本テレビ『ダイエット★ヴィレッジ』,,,どんなおデブでも（もちろん普通の人でも）結果が出る、やせる、人生が変わる！超おデブたちに連帯責任を課し、体重を落とし続けてきた日本で一番パワフルなダイエット番組、『ダイエット★ヴィレッジ』のメソッドを集大成。\n",
      "2\n",
      "【輸入楽譜】パルフーバー, Hermann: ヴィレッジ・フェスティバル: スコアとパート譜セット,パルフーバー, Hermann,【輸入楽譜】パルフーバー, Hermann: ヴィレッジ・フェスティバル: スコアとパート譜セット,パルフーバー, Hermann,【輸入楽譜】パルフーバー, Hermann: ヴィレッジ・フェスティバル: スコアとパート譜セット,パルフーバー, Hermann,【輸入楽譜】パルフーバー, Hermann: ヴィレッジ・フェスティバル: スコアとパート譜セット,パルフーバー, Hermann,【輸入楽譜】パルフーバー, Hermann: ヴィレッジ・フェスティバル: スコアとパート譜セット,パルフーバー, Hermann,,,\n",
      "3\n",
      "ヴィレッジフォン,佐藤彰男/イフティカル・ウディン・チョドリ,ヴィレッジフォン,佐藤彰男/イフティカル・ウディン・チョドリ,ヴィレッジフォン,佐藤彰男/イフティカル・ウディン・チョドリ,ヴィレッジフォン,佐藤彰男/イフティカル・ウディン・チョドリ,ヴィレッジフォン,佐藤彰男/イフティカル・ウディン・チョドリ,,グラミン銀行によるマイクロファイナンス事業と途上国,グラミン銀行のヴィレッジフォン・プログラムは、農村の貧しい女性たちに、救いの手をさしのべる事業となるはずであった。マスメディアが伝え得なかった実態を、詳細な現地調査により明らかにする。マイクロクレジットを活用した社会開発プロジェクトの効果と問題点に迫る。\n",
      "-----\n",
      "-----\n",
      "<<トレンド情報>>\n",
      "藤田紀子が貴乃花光司さんと絶縁に至った背景 「うまくお互い理解しあってうち解けて…」生島ヒロシはエール\n",
      "-----\n",
      "<<関連書籍>>\n",
      "1\n",
      "コトバノケイコ,河野景子,コトバノケイコ,河野景子,コトバノケイコ,河野景子,コトバノケイコ,河野景子,コトバノケイコ,河野景子,,,日々のコミュニケーションを楽しくするコトバの新解釈！コトバノケイコ！「日本は“結ぶ”文化です」「したたかは何故“強い”のか？」あなたの人生を変える令和のコトバ学！\n",
      "2\n",
      "さすが！と言われる 心に響く名スピーチのコツ＆実例集,生島 ヒロシ,さすが！と言われる 心に響く名スピーチのコツ＆実例集,生島 ヒロシ,さすが！と言われる 心に響く名スピーチのコツ＆実例集,生島 ヒロシ,さすが！と言われる 心に響く名スピーチのコツ＆実例集,生島 ヒロシ,さすが！と言われる 心に響く名スピーチのコツ＆実例集,生島 ヒロシ,,語りの名人が教える 短くても感銘を与えるあいさつ,誰でもスピーチ名人になれる、そのまま使える「名言」、効果的な「フレーズ」満載！語りの名人が教える、短くても感銘を与えるあいさつ。\n",
      "3\n",
      "北の富士流,村松 友視,北の富士流,村松 友視,北の富士流,村松 友視,北の富士流,村松 友視,北の富士流,村松 友視,,,いまや「日本一着物が似合う男」としてＮＨＫの大相撲解説で人気の北の富士勝昭。第５２代横綱であり、親方として千代の富士、北勝海の二人の横綱を育て上げた名伯楽でもあります。そんな北の富士の男としての魅力を余すところなく綴ったのが本書であります。老若男女全ての大相撲ファン、いや全日本国民、必読の一冊！\n",
      "-----\n",
      "-----\n",
      "<<トレンド情報>>\n",
      "＜柄本佑＞「柄本佑はイケメンか否か？」論争に“物申す” 大好きなギャル文化に持論も 「A-Studio＋」で\n",
      "-----\n",
      "<<関連書籍>>\n",
      "1\n",
      "UN539　エヴォケーション／エレビー　（Studio）,,UN539　エヴォケーション／エレビー　（Studio）,,UN539　エヴォケーション／エレビー　（Studio）,,UN539　エヴォケーション／エレビー　（Studio）,,UN539　エヴォケーション／エレビー　（Studio）,,,,\n",
      "2\n",
      "Studio　Ghibli　Songs　for　Solo　Ukulele（1）,,Studio　Ghibli　Songs　for　Solo　Ukulele（1）,,Studio　Ghibli　Songs　for　Solo　Ukulele（1）,,Studio　Ghibli　Songs　for　Solo　Ukulele（1）,,Studio　Ghibli　Songs　for　Solo　Ukulele（1）,,,【英語版】ウクレレでやさしく弾けるスタジオジブリ,\n",
      "3\n",
      "ディズニー物語にでてくるおいしいレシピ,ABC　Cooking　Studio,ディズニー物語にでてくるおいしいレシピ,ABC　Cooking　Studio,ディズニー物語にでてくるおいしいレシピ,ABC　Cooking　Studio,ディズニー物語にでてくるおいしいレシピ,ABC　Cooking　Studio,ディズニー物語にでてくるおいしいレシピ,ABC　Cooking　Studio,,,ディズニーワールドを作って楽しめる！１３コの物語が登場、全６０品のかわいいレシピ。\n",
      "-----\n",
      "-----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<トレンド情報>>\n",
      "東京事変の2021年新曲第1弾“闇なる白”が明日配信リリース！吉澤智子書き下ろしNHKドラマ『ドリームチーム』主題歌\n",
      "-----\n",
      "<<関連書籍>>\n",
      "1\n",
      "僕らだけの主題歌,,僕らだけの主題歌,,僕らだけの主題歌,,僕らだけの主題歌,,僕らだけの主題歌,,,,\n",
      "2\n",
      "LTBS173　FOUL／東京事変,,LTBS173　FOUL／東京事変,,LTBS173　FOUL／東京事変,,LTBS173　FOUL／東京事変,,LTBS173　FOUL／東京事変,,,,\n",
      "3\n",
      "邦画の主題歌による合唱はつらいよ,,邦画の主題歌による合唱はつらいよ,,邦画の主題歌による合唱はつらいよ,,邦画の主題歌による合唱はつらいよ,,邦画の主題歌による合唱はつらいよ,,,,\n",
      "-----\n",
      "-----\n",
      "<<トレンド情報>>\n",
      "ＵＦＣマクレガー復帰戦へ「ポワリエを２度倒す」\n",
      "-----\n",
      "<<関連書籍>>\n",
      "1\n",
      "マクレガーとホークの世代,石原郁子,マクレガーとホークの世代,石原郁子,マクレガーとホークの世代,石原郁子,マクレガーとホークの世代,石原郁子,マクレガーとホークの世代,石原郁子,,ユアン・マクレガー　スティーヴン・ドーフ　イーサン,\n",
      "2\n",
      "青木真也の柔道＆柔術入門,青木真也,青木真也の柔道＆柔術入門,青木真也,青木真也の柔道＆柔術入門,青木真也,青木真也の柔道＆柔術入門,青木真也,青木真也の柔道＆柔術入門,青木真也,,The　Finish！！,本書は青木真也が考える『組み技格闘技』を『道衣』を通して表現したものである。柔道、柔術、サンボ、サブミッションレスリング…すべての格闘技愛好家に贈る。\n",
      "3\n",
      "戦略の形成　下,ウィリアムソン・マーレー/マクレガー・ノックス,戦略の形成　下,ウィリアムソン・マーレー/マクレガー・ノックス,戦略の形成　下,ウィリアムソン・マーレー/マクレガー・ノックス,戦略の形成　下,ウィリアムソン・マーレー/マクレガー・ノックス,戦略の形成　下,ウィリアムソン・マーレー/マクレガー・ノックス,,支配者、国家、戦争,戦略の策定を論じる際、しばしばクラウゼヴィッツやリデルハート等、戦略思想家の影響が語られてきたが、本書はこれに疑問を呈する。戦略とは、敵・味方の相互作用であり、不可測な要素が支配する領域であるので、明確で論理的な原理や原則は存在し得ないと指摘する。地理や歴史、世界観や経済などの多様な要因を丁寧に検討することによってはじめて、「戦略」というものの全体像が浮かび上がってくるのである。その具体例として、下巻ではナチス・ドイツ、イスラエルから第二次世界大戦後のアメリカまでの事例を収録。\n",
      "-----\n",
      "-----\n",
      "<<トレンド情報>>\n",
      "THE YELLOW MONKEY、ドームツアー3公演の楽曲を同時に視聴できるティザー映像公開\n",
      "-----\n",
      "<<関連書籍>>\n",
      "1\n",
      "THE　YELLOW　MONKEY　sicks,,THE　YELLOW　MONKEY　sicks,,THE　YELLOW　MONKEY　sicks,,THE　YELLOW　MONKEY　sicks,,THE　YELLOW　MONKEY　sicks,,,,\n",
      "2\n",
      "LBS1　SPARK／THE　YELLOW　MONKEY,,LBS1　SPARK／THE　YELLOW　MONKEY,,LBS1　SPARK／THE　YELLOW　MONKEY,,LBS1　SPARK／THE　YELLOW　MONKEY,,LBS1　SPARK／THE　YELLOW　MONKEY,,,,\n",
      "3\n",
      "THE　YELLOW　MONKEY　sicks,,THE　YELLOW　MONKEY　sicks,,THE　YELLOW　MONKEY　sicks,,THE　YELLOW　MONKEY　sicks,,THE　YELLOW　MONKEY　sicks,,,,\n",
      "-----\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# トレンドに関連した商品を抽出\n",
    "for i,u in enumerate(test_item):\n",
    "    similarity = cosine_similarity(u, X[:-20])\n",
    "    similarity_list = similarity[0].tolist()\n",
    "   \n",
    "    print(\"<<トレンド情報>>\")\n",
    "    print(google_trends[\"google_abst\"][i])\n",
    "    print('-----')   \n",
    "    print(\"<<関連書籍>>\")\n",
    "    \n",
    "    tmp_max_value = 0\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    for v in range(-1,-4,-1):\n",
    "        max_value = sorted(similarity_list)[v]\n",
    "        index_nums = [n for n, v in enumerate(similarity_list) if v == max_value]\n",
    "        \n",
    "        for index_num in index_nums:\n",
    "            print(count+1)\n",
    "            print(df3.loc[index_num, \"test\"])\n",
    "            \n",
    "            count = count + 1\n",
    "            \n",
    "            if count == 3:\n",
    "                break\n",
    "        \n",
    "        if count == 3:\n",
    "            count = 0\n",
    "            break\n",
    "            \n",
    "\n",
    "    print('-----')\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トレンド情報：　 米大リーグ歴代２位の７５５本塁打、ハンク・アーロンさん死去…８６歳\n",
      "特徴語：　 Index(['ハンク', 'アトランタ', 'アルビレックスbc'], dtype='object')\n",
      "---\n",
      "トレンド情報：　 坂上忍 昨年結婚、野呂佳代の「僕が婚姻届の証人です」夫の正体も明かし松本「知らんかった」\n",
      "特徴語：　 Index(['野呂佳代', '真木よう子', 'ダウンタウンなう'], dtype='object')\n",
      "---\n",
      "トレンド情報：　 岩田剛典 “I Don't Like Mondays.” YU、幼馴染対談実現に「記念すべき日」\n",
      "特徴語：　 Index(['名も無き世界のエンドロール', 'don', 'like'], dtype='object')\n",
      "---\n",
      "トレンド情報：　 miHoYoの『崩壊3rd』がApp Store売上ランキングで327位→19位に急上昇 コラボ戦乙女にアスカ登場の「エヴァンゲリオン」コラボ開催で\n",
      "特徴語：　 Index(['エヴァンゲリオン', 'アスカ', 'ラングレー'], dtype='object')\n",
      "---\n",
      "トレンド情報：　 元欅坂46今泉佑唯ファンがワタナベマホトとの結婚を祝福できない理由\n",
      "特徴語：　 Index(['ワタナベマホト', '今泉佑唯', 'https'], dtype='object')\n",
      "---\n",
      "トレンド情報：　 アイナ・ジ・エンド、「THE FIRST TAKE」第86回にて“オーケストラ”を披露\n",
      "特徴語：　 Index(['エンド', 'first', 'take'], dtype='object')\n",
      "---\n",
      "トレンド情報：　 応援購入サービス「Makuake」、「オンライン催事」機能の提供を開始！ ～第一弾は大丸東京店セレクトの銘菓子店を集めた「オンライン催事・大丸東京店 ...\n",
      "特徴語：　 Index(['makuake', 'https', 'jp'], dtype='object')\n",
      "---\n",
      "トレンド情報：　 “コンプライアンス”違反を指摘する視聴者からの手紙が引き金となり、過剰な役柄の滝藤賢一は降板させられる事態に...：バイプレイヤーズ\n",
      "特徴語：　 Index(['観月ありさ', 'バイプレイヤーズ', '大倉孝二'], dtype='object')\n",
      "---\n",
      "トレンド情報：　 IBMの第4四半期売上高は予想下回る ソフト事業が不振\n",
      "特徴語：　 Index(['ibm', 'intel', 'in'], dtype='object')\n",
      "---\n",
      "トレンド情報：　 邪悪と狂気に満ちる村『バイオハザード ヴィレッジ』5.8発売決定 トレーラー＆キャラ写真公開\n",
      "特徴語：　 Index(['ヴィレッジ', 'バイオハザード', 'ドミトレスク'], dtype='object')\n",
      "---\n",
      "トレンド情報：　 藤田紀子が貴乃花光司さんと絶縁に至った背景 「うまくお互い理解しあってうち解けて…」生島ヒロシはエール\n",
      "特徴語：　 Index(['藤田紀子', '貴乃花', '花田優一'], dtype='object')\n",
      "---\n",
      "トレンド情報：　 ＜柄本佑＞「柄本佑はイケメンか否か？」論争に“物申す” 大好きなギャル文化に持論も 「A-Studio＋」で\n",
      "特徴語：　 Index(['studio', '柄本佑', '奥田瑛二'], dtype='object')\n",
      "---\n",
      "トレンド情報：　 東京事変の2021年新曲第1弾“闇なる白”が明日配信リリース！吉澤智子書き下ろしNHKドラマ『ドリームチーム』主題歌\n",
      "特徴語：　 Index(['ドリームチーム', '主題歌', '東京事変'], dtype='object')\n",
      "---\n",
      "トレンド情報：　 ＵＦＣマクレガー復帰戦へ「ポワリエを２度倒す」\n",
      "特徴語：　 Index(['ポイエー', 'マクレガー', 'ufc'], dtype='object')\n",
      "---\n",
      "トレンド情報：　 THE YELLOW MONKEY、ドームツアー3公演の楽曲を同時に視聴できるティザー映像公開\n",
      "特徴語：　 Index(['yellow', 'monkey', 'バラ色の日々'], dtype='object')\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(feature_list)):\n",
    "    print(\"トレンド情報：　\", google_trends[\"google_abst\"][i])\n",
    "    print(\"特徴語：　\",feature_list[i].index)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 書籍データを蓄積するリストを用意\n",
    "search_list = {}\n",
    "\n",
    "url = \"https://app.rakuten.co.jp/services/api/BooksTotal/Search/20170404\"\n",
    "\n",
    "# APIでデータ集計を行うページ番号について、初期値を設定\n",
    "page_num = 1\n",
    "\n",
    "for i in range(len(feature_list)):\n",
    "    for k,feature_name in enumerate(feature_list[i].index):\n",
    "\n",
    "        # URLに付与するパラメータ\n",
    "        payload = {\n",
    "            'applicationId': config[\"RAKUTEN_API\"][\"id\"],\n",
    "            'hits': 5,\n",
    "            'page': page_num,\n",
    "            'formatVersion': 2,\n",
    "            'keyword': feature_name\n",
    "        }\n",
    "\n",
    "        r = requests.get(url, params=payload) \n",
    "        resp = r.json()\n",
    "        for j in range(len(resp[\"Items\"])):\n",
    "            resp[\"Items\"][j][\"rank\"] = i + 1\n",
    "            search_list[feature_name] = resp['Items']\n",
    "\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<各トレンドの文章に含まれる上位の特徴語と、特徴語で楽天APIの検索をかけた結果>>\n",
      "-----\n",
      "ハンク : デュオ\n",
      "アトランタ : アトランタ・ノースカロライナ・サウスカロライナ・テネシー・アラバマ・テキサス便利（VOL．16）\n",
      "-----\n",
      "野呂佳代 : ハッピーメール\n",
      "真木よう子 : 別冊月刊真木よう子Noise\n",
      "-----\n",
      "名も無き世界のエンドロール : 名も無き世界のエンドロール\n",
      "don : 【楽天ブックス限定先着特典】AGE OF ZOC/DON'T TRUST TEENAGER (CD＋DVD＋スマプラ)(特典内容未定)\n",
      "like : 春雷 feat. 露崎春女\n",
      "-----\n",
      "エヴァンゲリオン : 新世紀エヴァンゲリオン原画集ダイジェスト\n",
      "アスカ : 百希夜行 (完全生産限定盤 CD＋Blu-ray＋Photobook)\n",
      "ラングレー : Figuarts mini 式波・アスカ・ラングレー\n",
      "-----\n",
      "ワタナベマホト : にゃにゃにゃの本〜YouTuber・ワタナベマホトと猫たち〜\n",
      "今泉佑唯 : 酔うと化け物になる父がつらい\n",
      "-----\n",
      "https : ホームページ改善の解決メソッド37\n",
      "-----\n",
      "エンド : エンドロールバック（3）（完）\n",
      "first : PSYCHO-PASS サイコパス3 FIRST INSPECTOR【Blu-ray】\n",
      "take : GIVE　＆　TAKE\n",
      "-----\n",
      "makuake : Deff WIZ Wireless Charging Tray Qi 最大15W キャメルブラウン\n",
      "jp : コンフィデンスマンJP プリンセス編 超豪華版【Blu-ray】\n",
      "-----\n",
      "観月ありさ : VINGT-CINQ ANS (3CD)\n",
      "バイプレイヤーズ : SODA (ソーダ) 2021年3月号(表紙:松村北斗)\n",
      "大倉孝二 : 見えない目撃者\n",
      "-----\n",
      "ibm : THE DX\n",
      "intel : ＜BXNUC10I7FNH ＞第10世代Corei7-10710U（1.1-4.7GHz/6 Core/Intel UHD Graphics）搭載NUCキット、 M.2スロット and 2.5 Drive\n",
      "in : 【楽天ブックス限定先着特典+先着特典】POP IN CITY ～for covers only～ (初回限定盤 CD＋Blu-ray)(オリジナルA5ミニクリアファイル+オンラインイベント応募はがき)\n",
      "-----\n",
      "ヴィレッジ : 【特典】BIOHAZARD VILLAGE Z Version PS5版(数量限定封入特典：武器パーツ「ラクーン君」と「サバイバルリソースパック」が手に入るプロダクトコード)\n",
      "バイオハザード : 【特典】BIOHAZARD VILLAGE Z Version PS5版(数量限定封入特典：武器パーツ「ラクーン君」と「サバイバルリソースパック」が手に入るプロダクトコード)\n",
      "ドミトレスク : アイ・コンセントレイト・オン・ユー\n",
      "-----\n",
      "藤田紀子 : 四国ガチンコ!お遍路道修繕プロジェクトCD\n",
      "貴乃花 : 貴乃花 我が相撲道\n",
      "花田優一 : みなさん どうぞ\n",
      "-----\n",
      "studio : Studio One 4ガイドブック\n",
      "柄本佑 : 心の傷を癒すということ\n",
      "奥田瑛二 : 洗骨\n",
      "-----\n",
      "ドリームチーム : アリンコの唄/騒Get!\n",
      "主題歌 : 【先着特典】TVアニメ「SHAMAN KING」主題歌マキシシングル(仮)(L判ブロマイド)\n",
      "東京事変 : ニュース\n",
      "-----\n",
      "ポイエー : 日本語類義表現と使い方のポイントー表現意図から考えるー\n",
      "マクレガー : ドクター・スリープ【Blu-ray】\n",
      "ufc : EA SPORTS UFC 4\n",
      "-----\n",
      "yellow : 【楽天ブックス限定先着特典】30th Anniversary THE YELLOW MONKEY SUPER DOME TOUR BOX【Blu-ray】(オリジナル・カラビナ)\n",
      "monkey : 【楽天ブックス限定先着特典】30th Anniversary THE YELLOW MONKEY SUPER DOME TOUR BOX【Blu-ray】(オリジナル・カラビナ)\n",
      "バラ色の日々 : My Generation/あぁ バラ色の日々\n"
     ]
    }
   ],
   "source": [
    "tmp = 1\n",
    "print(\"<<各トレンドの文章に含まれる上位の特徴語と、特徴語で楽天APIの検索をかけた結果>>\")\n",
    "\n",
    "print(\"-----\")\n",
    "for k,v in search_list.items():\n",
    "    if v[0][\"rank\"] != tmp:\n",
    "        print(\"-----\") \n",
    "    print(k,\":\",v[0][\"title\"])\n",
    "    tmp = v[0][\"rank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
